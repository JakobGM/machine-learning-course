{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization,Dropout, Embedding, Flatten, Concatenate, Input, Lambda, PReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from scipy.special import erfinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_datasets\n",
    "datasets = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankGaussScalar(object):\n",
    "    def __init__(self):\n",
    "        self.fit_done = False\n",
    "\n",
    "    def rank_gauss(self, x):\n",
    "        N = x.shape[0]\n",
    "        temp = x.argsort()\n",
    "        rank_x = temp.argsort() / N\n",
    "        rank_x -= rank_x.mean()\n",
    "        rank_x *= 2\n",
    "        efi_x = erfinv(rank_x)\n",
    "        efi_x -= efi_x.mean()\n",
    "        return efi_x\n",
    "\n",
    "    def fit(self, df_x):\n",
    "        self.train_unique_rankgauss = {}\n",
    "        self.target_cols = np.sort(df_x.columns)\n",
    "        for c in self.target_cols:\n",
    "            unique_val = np.sort(df_x[c].unique())\n",
    "            self.train_unique_rankgauss[c]= [unique_val, self.rank_gauss(unique_val)]\n",
    "        self.fit_done = True\n",
    "\n",
    "    def transform(self, df_target):\n",
    "        assert self.fit_done\n",
    "        assert np.all(np.sort(np.intersect1d(df_target.columns, self.target_cols)) == np.sort(self.target_cols))\n",
    "        df_converted_rank_gauss = pd.DataFrame(index=df_target.index)\n",
    "        for c in self.target_cols:\n",
    "            df_converted_rank_gauss[c] = np.interp(df_target[c], \n",
    "                                                   self.train_unique_rankgauss[c][0], \n",
    "                                                   self.train_unique_rankgauss[c][1]) # ,left=0, right=0)\n",
    "        return df_converted_rank_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model_input = Input(shape=(6,))\n",
    "    num_input = Lambda(lambda x: x[:, :5], output_shape=(5,))(model_input)\n",
    "    cat_input = Lambda(lambda x: x[:, 5:], output_shape=(1,))(model_input)\n",
    "    \n",
    "    x = num_input\n",
    "    x = Dense(256)(num_input)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "\n",
    "    emb_dim = 128\n",
    "    y = Embedding(200, emb_dim, input_length=1)(cat_input)\n",
    "    y = Flatten()(y)\n",
    "    \n",
    "    z = Concatenate()([x, y])\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(rate=0.5)(z)\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(rate=0.5)(z)\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    output = Dense(1, activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=model_input, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_dataset(df, cnum):\n",
    "    _dset = df.filter(regex=f'var_{cnum}$')\n",
    "    _dset.columns = list(range(_dset.shape[1]))\n",
    "    _dset = _dset.assign(var_num = cnum)\n",
    "    return _dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model_output_dir = f'../processed/nn_output/'\n",
    "    if not os.path.isdir(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "    dataset_dir = '../processed/dataset/'\n",
    "    X_train = pd.read_pickle(os.path.join(dataset_dir, 'X_train.pickle'))\n",
    "    y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "    X_test = pd.read_pickle(os.path.join(dataset_dir, 'X_test.pickle'))\n",
    "\n",
    "    epochs = 40\n",
    "    batch_size = 1024\n",
    "    patience = 5\n",
    "\n",
    "    dset_list = []\n",
    "    for cnum in range(200):\n",
    "    _dset = arrange_dataset(X_train, cnum)\n",
    "    dset_list.append(_dset)\n",
    "    concat_X_train = pd.concat(dset_list, axis=0)\n",
    "    train_dset = [concat_X_train, pd.concat([y_train for c in range(200)], axis=0)]\n",
    "\n",
    "    weights_dir = '../processed/keras_weights'\n",
    "    if not os.path.isdir(weights_dir):\n",
    "    os.makedirs(weights_dir)\n",
    "\n",
    "    for fold_set_number in range(10):\n",
    "    print('### start iter {} in 10 ###'.format(fold_set_number+1))\n",
    "    K.clear_session()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019+fold_set_number)\n",
    "    folds = [\n",
    "    [\n",
    "        np.concatenate([_trn+i * X_train.shape[0] for i in range(200)]), \n",
    "        np.concatenate([_val+i * X_train.shape[0] for i in range(200)])\n",
    "    ] for _trn, _val in skf.split(X_train, y_train)]\n",
    "\n",
    "    for fold_num in range(5):\n",
    "    print(f'## Start KFold number {fold_num} ##')\n",
    "    model_management_num = fold_num + fold_set_number*5\n",
    "    skf_train_index, skf_valid_index = folds[fold_num]\n",
    "\n",
    "    nonprogress_counter=0\n",
    "    e_auc_best = 0\n",
    "\n",
    "    weight_path = os.path.join(weights_dir, f'{model_management_num}.model')\n",
    "    skf_X_train = train_dset[0].iloc[skf_train_index].copy()\n",
    "    skf_y_train = train_dset[1].iloc[skf_train_index]\n",
    "    skf_X_valid = train_dset[0].iloc[skf_valid_index].copy()\n",
    "    skf_y_valid = train_dset[1].iloc[skf_valid_index]\n",
    "    single_valid_index = skf_valid_index[:skf_valid_index.shape[0]//200]  \n",
    "\n",
    "    rgscaler = RankGaussScalar()\n",
    "    rgscaler.fit(skf_X_train.iloc[:, :5].astype(float))\n",
    "    skf_X_train.iloc[:, :5] = rgscaler.transform(skf_X_train.iloc[:, :5].astype(float))\n",
    "    skf_X_valid.iloc[:, :5] = rgscaler.transform(skf_X_valid.iloc[:, :5].astype(float))\n",
    "\n",
    "    print('start training. ')\n",
    "    for _e in range(epochs):\n",
    "        print('epoch {}'.format(_e))\n",
    "        if _e == 0:\n",
    "            model = build_model()\n",
    "            #optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)\n",
    "            optimizer = optimizers.adam(lr=0.001)\n",
    "            model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        history = model.fit(skf_X_train.values, skf_y_train,\n",
    "                        validation_data=[skf_X_valid.values, skf_y_valid], \n",
    "                        epochs=1,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "        oof_pred_array = np.ones((single_valid_index.shape[0], 200))\n",
    "        for cnum in range(200):\n",
    "            oof_pred_array[:, cnum] = np.squeeze(\n",
    "                model.predict(\n",
    "                    skf_X_valid.iloc[cnum*single_valid_index.shape[0]:(cnum+1)*single_valid_index.shape[0]].values, batch_size=100000\n",
    "                )\n",
    "            )\n",
    "        e_auc = roc_auc_score(y_train.iloc[single_valid_index], oof_pred_array.prod(axis=1))\n",
    "\n",
    "        print('\\tauc : {0:.6f}'.format(e_auc))\n",
    "        if e_auc > e_auc_best:\n",
    "            model.save_weights(weight_path)\n",
    "            e_auc_best = e_auc\n",
    "            nonprogress_counter = 0\n",
    "        else:\n",
    "            nonprogress_counter += 1\n",
    "\n",
    "        if (nonprogress_counter >= patience) or (_e == (epochs-1)):\n",
    "            print('fold end. ')\n",
    "            break\n",
    "    print('training end. ')\n",
    "\n",
    "    model.load_weights(weight_path)\n",
    "\n",
    "    print('start predicting. ')\n",
    "    oof_pred_array = np.ones((single_valid_index.shape[0], 200))\n",
    "    test_pred_array = np.ones((X_test.shape[0], 200))\n",
    "    for cnum in range(200):\n",
    "        tmp_X_test = arrange_dataset(X_test, cnum).copy()\n",
    "        tmp_X_test.iloc[:, :5] = rgscaler.transform(tmp_X_test.iloc[:, :5].astype(float))\n",
    "        oof_pred_array[:, cnum] = np.squeeze(\n",
    "            model.predict(\n",
    "                skf_X_valid.iloc[cnum*single_valid_index.shape[0]:(cnum+1)*single_valid_index.shape[0]].values, batch_size=100000\n",
    "            )\n",
    "        )\n",
    "        test_pred_array[:, cnum] = np.squeeze(model.predict(tmp_X_test.values, batch_size=100000))\n",
    "    fold_oof_pred = pd.DataFrame(oof_pred_array, index=X_train.index[single_valid_index])\n",
    "    fold_test_pred = pd.DataFrame(test_pred_array, index=X_test.index)\n",
    "    print('prediction end. ')\n",
    "\n",
    "    print('save fold results')\n",
    "    fold_oof_pred.to_pickle(os.path.join(model_output_dir, f'oof_preds_{model_management_num}.pkl.gz'), compression='gzip')\n",
    "    fold_test_pred.to_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'), compression='gzip')\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
