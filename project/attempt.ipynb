{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipypb import irange, track\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from datasets import get_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by assembling the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only train and predict on the small train and test sets, respectively, so let's retrieve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the train and test `ID_code` values in order to distinguish between these two datasets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = dataset.all_train.X.index\n",
    "test_ids = dataset.unlabeled_test.X.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the feature engineering upon the _all_ the features in the dataset, i.e. train and test features.\n",
    "Let's concatenate these two feature sets, we can retrieve the correct split later on with `train_ids` and `test_ids` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = dataset.all_train.X\n",
    "test_features = dataset.unlabeled_test.X\n",
    "all_features = pd.concat([train_features, test_features], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle user Chris Deotte plotted the feature probability densities [here](https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899/notebook).\n",
    "\"Sibmike\" later observed that these probability graphs are quite similar, as mentioned in his notebook [here](https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals).\n",
    "The only difference is that a fraction of the feature densities are flipped left/right.\n",
    "By experimentation, it has been shown that if you flip all the graph such that they portray the same similar shape, LightGBM models perform better.\n",
    "By arbitrary choice, we will flip those densities which have their modes on the left half side such that they have their modes on the right hand side.\n",
    "This can be thought of as a data normalization.\n",
    "This is implemented below, where `indeces` are those features that have been identified as being \"reflected\" wrt. the remaining indeces, found by visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    indeces = [\n",
    "        0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 15, 16, 18, 19, 22, 24, 25, 26, 27,\n",
    "        41, 29, 32, 35, 37, 40, 48, 49, 47, 55, 51, 52, 53, 60, 61, 62, 103,\n",
    "        65, 66, 67, 69, 70, 71, 74, 78, 79, 82, 84, 89, 90, 91, 94, 95, 96,\n",
    "        97, 99, 105, 106, 110, 111, 112, 118, 119, 125, 128, 130, 133, 134,\n",
    "        135, 137, 138, 140, 144, 145, 147, 151, 155, 157, 159, 161, 162, 163,\n",
    "        164, 167, 168, 170, 171, 173, 175, 176, 179, 180, 181, 184, 185, 187,\n",
    "        189, 190, 191, 195, 196, 199]\n",
    "    column_names = [f\"var_{index}\" for index in indeces]\n",
    "    for column_name in column_names:\n",
    "        dataframe[column_name] *= -1\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform this feature engineering on both the training and test features, and we are doing this in-place, forgetting about the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_features = reverse(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a pretty standard data normalization technique, subtracting the feature mean and dividing by the standard deviation.\n",
    "\n",
    "$$\n",
    "    feature \\leftarrow \\frac{feature - \\mathrm{mean}(feature)}{\\mathrm{SD}(feature)}\n",
    "$$\n",
    "\n",
    "This results in normalized features with mean `0` and standard deviation / variance equal to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    for column in dataframe:\n",
    "        mean = dataframe[column].mean()\n",
    "        std = dataframe[column].std()\n",
    "        dataframe[column] = (dataframe[column] - mean) / std\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we apply this to both feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scale(reversed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to one of the most important feature engineering tricks for the Santander Transaction Prediction dataset, used by every single top-performing entry in the original leaderboard, **feature uniqueness**.\n",
    "This time, we will not take the `target` value into account at all, only focusing on feature uniqueness.\n",
    "For every single original (now normalized) feature, we will calculate the number of exact duplicates.\n",
    "For instance for `var_0` we will calculate a new `var_0_count` columns which is set to the number of duplicates found for that specific value of `var_0`.\n",
    "Every row that shares a value for `var_0` will therefore share the same value of `var_0_count`, and `var_0_count` will be equal to the number of rows that share that specific value of `var_0`.\n",
    "Let's implement this in `add_count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count(dataframe):\n",
    "    new_dataframe = dataframe.copy()\n",
    "    \n",
    "    for column in track(list(dataframe.columns)):\n",
    "        count = dataframe[column].value_counts().to_dict()\n",
    "        new_dataframe[str(column) + \"_count\"] = dataframe[column].map(count)\n",
    "        \n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply this function to both splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[00:21<00:00, 0.10s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [00:21<00:00, 0.10s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_features = add_count(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we will not stop here, as we did with the previous feature engineering.\n",
    "This time we will add a degree of fuzzyness to the criterion of being \"non-unique\".\n",
    "This is the first of two main ideas we will borrow from the 2nd place solution to the original competition, which can be found [here](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/88939).\n",
    "The idea is to round the features to a given number of digits before finding feature duplicates, everything else being equal to `add_count()` defined above.\n",
    "We will round to `2` and `3` digits and call these two new columns per original covariate for `var_X_rounded_count_2` and `var_X_rounded_count_3` for X = 0, 1, 2, ..., 199."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_round(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    columns = [f\"var_{index}\" for index in range(0, 200)]\n",
    "    dataframe = dataframe.copy()\n",
    "    for digits in [4, 3, 2]:\n",
    "        for column in track(columns):\n",
    "            rounded = dataframe[column].round(decimals=digits)\n",
    "            count = rounded.value_counts().to_dict()\n",
    "            dataframe[str(column) + f\"_rounded_count_{digits}\"] = rounded.map(count)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW attepmt\n",
    "\n",
    "def add_round(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    columns = [f\"var_{index}\" for index in range(0, 200)]\n",
    "    dataframe = dataframe.copy()\n",
    "    for digits in [4, 3, 2]:\n",
    "        for column in track(columns):\n",
    "            rounded = dataframe[column].round(decimals=digits)\n",
    "            count = rounded.value_counts().to_dict()\n",
    "            dataframe[str(column) + f\"_rounded_count_{digits}\"] = rounded.map(count)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we apply this function to our dataset, resulting in two new columns per original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[00:12<00:00, 0.06s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [00:12<00:00, 0.06s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[00:03<00:00, 0.02s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [00:03<00:00, 0.02s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[00:02<00:00, 0.01s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [00:02<00:00, 0.01s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rounded_count_features = add_round(count_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now retrieve the original training and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = rounded_count_features.loc[train_ids]\n",
    "test_features = rounded_count_features.loc[test_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also retrieve the targets for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = dataset.all_train.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the second main idea of the 2nd place solution from \"Onodera\" which we have had good experience in adopting for our smaller dataset.\n",
    "The idea is to split every row into 200 separate rows, each new row solely containing features related to the same original feature.\n",
    "For instance one row will contain `var_0`, `var_0_count`, `var_0_rounded_count_2`, `var_0_rounded_count_3`, all continuous features, and one last categorical covariate `var` which will be a categorical feature equal to `0` in the given example.\n",
    "\n",
    "We start by defining a function which filters down a given dataset to only have those columns related to `variable` (e.g. `0`) and then adds the categorical column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset, variable):\n",
    "    dataset = dataset.copy()\n",
    "    dataset = dataset.filter(regex=f\"(^var_{variable}$|^var_{variable}_)\")\n",
    "    dataset.columns = list(range(dataset.shape[1]))\n",
    "    dataset = dataset.assign(var=variable)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a helper function which creates such a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_features(dataset):\n",
    "    dataset = dataset.copy()\n",
    "    categorical = pd.concat(\n",
    "        [filter_dataset(dataset, variable) for variable in irange(0, 200)],\n",
    "        axis=0,\n",
    "        sort=False,\n",
    "    )\n",
    "    categorical[\"var_num\"] = categorical[\"var\"].astype(\"category\")\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's apply this to our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[00:30<00:00, 0.15s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [00:30<00:00, 0.15s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "categorical_train_features = categorical_features(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each original observation row has now been duplicated 200 times, one for each original feature.\n",
    "We will therefore need to duplicate the targets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_targets = pd.concat([train_targets] * 200, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a lightgbm `Dataset` object used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(\n",
    "    data=categorical_train_features,\n",
    "    label=duplicated_targets,\n",
    "    free_raw_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the LightGBM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'boost_from_average': 'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 1.0,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': -1,\n",
    "    'metric': 'binary_logloss',\n",
    "    'min_data_in_leaf': 30,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 64,\n",
    "    'num_threads': 24,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary',\n",
    "    'verbosity': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blog post given [here](https://blog.amedama.jp/entry/lightgbm-cv-model) authored by \"momijiame\" explains how to save the best iteration model\n",
    "when using the `lgb.cv` API. The source code is used in quite a lot of Kaggle notebooks, and we have taken the source code for the given callback from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExtractionCallback:\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def __call__(self, env):\n",
    "        self._model = env.model\n",
    "\n",
    "    def _assert_called_cb(self):\n",
    "        if self._model is None:\n",
    "            raise RuntimeError('callback has not called yet')\n",
    "\n",
    "    @property\n",
    "    def boosters_proxy(self):\n",
    "        self._assert_called_cb()\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def raw_boosters(self):\n",
    "        self._assert_called_cb()\n",
    "        return self._model.boosters\n",
    "\n",
    "    @property\n",
    "    def best_iteration(self):\n",
    "        self._assert_called_cb()\n",
    "        return self._model.best_iteration\n",
    "    \n",
    "callback = ModelExtractionCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified 5-fold cross validation for early stopping **and** for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "NUM_SPLITS = 5\n",
    "five_fold = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=42)\n",
    "folds = five_fold.split(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must do some index manipulation in order to translate these splits into the duplicated array.\n",
    "This is required such that the same observation but with different constructed covariates is not distributed across splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_folds = [\n",
    "    [\n",
    "        np.concatenate([train_idx + i * train_features.shape[0] for i in range(0, 200)]),\n",
    "        np.concatenate([val_idx + i * train_features.shape[0] for i in range(0, 200)]),\n",
    "    ] for train_idx, val_idx in folds\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform cross-validated, early stopping training using `lgb.cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tcv_agg's binary_logloss: 0.586308 + 6.42824e-06\n",
      "[40]\tcv_agg's binary_logloss: 0.512171 + 1.40957e-05\n",
      "[60]\tcv_agg's binary_logloss: 0.459488 + 1.90468e-05\n",
      "[80]\tcv_agg's binary_logloss: 0.421557 + 2.47794e-05\n",
      "[100]\tcv_agg's binary_logloss: 0.394077 + 3.13329e-05\n",
      "[120]\tcv_agg's binary_logloss: 0.374131 + 3.63861e-05\n",
      "[140]\tcv_agg's binary_logloss: 0.359676 + 4.09908e-05\n",
      "[160]\tcv_agg's binary_logloss: 0.34922 + 4.55035e-05\n",
      "[180]\tcv_agg's binary_logloss: 0.341683 + 5.1644e-05\n",
      "[200]\tcv_agg's binary_logloss: 0.336292 + 5.59257e-05\n",
      "[220]\tcv_agg's binary_logloss: 0.332462 + 5.99033e-05\n",
      "[240]\tcv_agg's binary_logloss: 0.32976 + 6.39555e-05\n",
      "[260]\tcv_agg's binary_logloss: 0.327864 + 6.73778e-05\n",
      "[280]\tcv_agg's binary_logloss: 0.326543 + 7.0417e-05\n",
      "[300]\tcv_agg's binary_logloss: 0.325627 + 7.30415e-05\n",
      "[320]\tcv_agg's binary_logloss: 0.324995 + 7.57341e-05\n",
      "[340]\tcv_agg's binary_logloss: 0.324562 + 7.80667e-05\n",
      "[360]\tcv_agg's binary_logloss: 0.324265 + 7.96655e-05\n",
      "[380]\tcv_agg's binary_logloss: 0.324063 + 8.09688e-05\n",
      "[400]\tcv_agg's binary_logloss: 0.323926 + 8.18694e-05\n",
      "[420]\tcv_agg's binary_logloss: 0.323833 + 8.26947e-05\n",
      "[440]\tcv_agg's binary_logloss: 0.323771 + 8.3389e-05\n",
      "[460]\tcv_agg's binary_logloss: 0.323729 + 8.39629e-05\n",
      "[480]\tcv_agg's binary_logloss: 0.323701 + 8.43721e-05\n",
      "[500]\tcv_agg's binary_logloss: 0.323682 + 8.49407e-05\n",
      "[520]\tcv_agg's binary_logloss: 0.323669 + 8.5106e-05\n",
      "[540]\tcv_agg's binary_logloss: 0.323661 + 8.538e-05\n",
      "[560]\tcv_agg's binary_logloss: 0.323656 + 8.55673e-05\n",
      "[580]\tcv_agg's binary_logloss: 0.323653 + 8.57321e-05\n",
      "[600]\tcv_agg's binary_logloss: 0.32365 + 8.5735e-05\n",
      "[620]\tcv_agg's binary_logloss: 0.323649 + 8.58867e-05\n",
      "[640]\tcv_agg's binary_logloss: 0.323649 + 8.5941e-05\n",
      "[660]\tcv_agg's binary_logloss: 0.323648 + 8.59419e-05\n",
      "[680]\tcv_agg's binary_logloss: 0.323648 + 8.61998e-05\n",
      "[700]\tcv_agg's binary_logloss: 0.323648 + 8.63313e-05\n"
     ]
    }
   ],
   "source": [
    "result = lgb.cv(\n",
    "    params=lgb_params, \n",
    "    train_set=lgb_train,\n",
    "    num_boost_round=100_000,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=20,\n",
    "    folds=modified_folds,\n",
    "    callbacks=[callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use all 5 models, one for each fold in order to make predictions for each of the covariate types (200).\n",
    "We also make out of fold predictions in order to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[08:58<00:03, 2.69s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [08:58<00:03, 2.69s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_models = callback.raw_boosters\n",
    "best_iteration = callback.best_iteration\n",
    "\n",
    "validation_predictions = np.ones((len(train_features), 200))\n",
    "test_predictions = np.ones((len(test_features), NUM_SPLITS, 200))\n",
    "\n",
    "for variable in irange(0, 200):\n",
    "    for fold, fold_model in enumerate(fold_models):\n",
    "        validation_indeces = fold_model.valid_sets[0].used_indices\n",
    "        validation_indeces = validation_indeces[: len(validation_indeces) // 200]\n",
    "        \n",
    "        validation_features = (\n",
    "            filter_dataset(train_features, variable)\n",
    "            .iloc[validation_indeces]\n",
    "            .values\n",
    "        )\n",
    "        validation_predictions[validation_indeces, variable] = fold_model.predict(\n",
    "            validation_features,\n",
    "            num_iteration=best_iteration,\n",
    "        )\n",
    "        \n",
    "        test_predictions[:, fold, variable] = fold_model.predict(\n",
    "            filter_dataset(test_features, variable).values,\n",
    "            num_iteration=best_iteration,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine each row that was originally split into 200 seperate rows, we combine each respective probability by calculating the products of all the odds, and the combining them together again.\n",
    "We will also remove covariates entirely from the model if the ROC AUC score compared with the training set is below 0.5, i.e. worse than completely guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"200\" value=\"200\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">200/200</span>\n",
       "<span class=\"Time-label\">[00:03<00:00, 0.02s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 200/200 [00:03<00:00, 0.02s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_odds = np.ones((train_features.shape[0]))\n",
    "test_odds = np.ones((test_features.shape[0], NUM_SPLITS))\n",
    "\n",
    "for variable in irange(0, 200):\n",
    "    variable_auc = roc_auc_score(\n",
    "        y_true=train_targets,\n",
    "        y_score=validation_predictions[:, variable],\n",
    "    )\n",
    "    if variable_auc >= 0.5:\n",
    "        preds = validation_predictions[:, variable]\n",
    "        validation_odds *= preds / (1 - preds)\n",
    "        \n",
    "        preds = test_predictions[:, :, variable]\n",
    "        test_odds *= preds / (1 - preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the ROC AUC for the out of fold predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-fold AUC = 0.8663901196471038\n"
     ]
    }
   ],
   "source": [
    "combined_validation_predictions = validation_predictions / (1 + validation_predictions)\n",
    "validation_auc = roc_auc_score(\n",
    "    y_true=train_targets,\n",
    "    y_score=combined_validation_predictions.mean(axis=1),\n",
    ")\n",
    "print(f\"Out-of-fold AUC = {validation_auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
