{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Machine Learning - Lecture September 19th, 2019\n",
    "\n",
    "Lecturer: Zhirong Yang\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "* We will focus on multiple real covariates and single real responses\n",
    "\n",
    "\n",
    "## Ensemble Learning (EL)\n",
    "\n",
    "* State-of-the-art\n",
    "* AdaBoost\n",
    "* Random Forest (try first this first)\n",
    "* XGBoost\n",
    "* CatBoost/LightGBM (quite new)\n",
    "* Deep Forest (quite new)\n",
    "\n",
    "What is ensemble learning?\n",
    "\n",
    "* Weighted average of \"weak learners\"\n",
    "* For binary classification we can use sgn(sum(weak learners))\n",
    "* Multi-class classification can use majority vote\n",
    "* In practice, almost all Kaggle winners use ensemble learning\n",
    "* Ensemble learning can decrease bias, e.g. by boosting\n",
    "* Ensemble learning can decrease variance, e.g. by bagging\n",
    "* Ensemble learning can make an overall improvement, e.g. by stacking\n",
    "\n",
    "How to create different learners?\n",
    "\n",
    "* Different learning algorithms\n",
    "* Different hyperparameters\n",
    "* Different representations\n",
    "* Different training sets\n",
    "* Artificial noise added to the data\n",
    "* Random samples from posterior of the model parameters (instead of finding the maximum)\n",
    "\n",
    "How to combine base learners?\n",
    "\n",
    "**Boosting**\n",
    "\n",
    "Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. Each model corrects the mistakes of its predecessor.\n",
    "\n",
    "**Bagging**\n",
    "\n",
    "Simultaneously construct a lot of the base models at the same time. The collection of base models vote on the final decision. The base models are distinguished by the datasets they have access to.\n",
    "\n",
    "**Stacking**\n",
    "\n",
    "Use predictions of a set of base learners as features in a higher-order model, a so called \"meta learner\".\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "The first practical boosting algorithm invented in 1995. Sequentially construct a set of base learners. Each sample has a weight, and the misclassified samples are emphasized more and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>767 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      6  148  72  35    0  33.6  0.627  50  1\n",
       "0     1   85  66  29    0  26.6  0.351  31  0\n",
       "1     8  183  64   0    0  23.3  0.672  32  1\n",
       "2     1   89  66  23   94  28.1  0.167  21  0\n",
       "3     0  137  40  35  168  43.1  2.288  33  1\n",
       "4     5  116  74   0    0  25.6  0.201  30  0\n",
       "..   ..  ...  ..  ..  ...   ...    ...  .. ..\n",
       "762  10  101  76  48  180  32.9  0.171  63  0\n",
       "763   2  122  70  27    0  36.8  0.340  27  0\n",
       "764   5  121  72  23  112  26.2  0.245  30  0\n",
       "765   1  126  60   0    0  30.1  0.349  47  1\n",
       "766   1   93  70  31    0  30.4  0.315  23  0\n",
       "\n",
       "[767 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.read_csv(\"pima-indians-diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is sensitive to outliers, and is slower than XGBoost.\n",
    "\n",
    "**Most popular type of base learners**\n",
    "\n",
    "* LogReg\n",
    "* Naive Bayes Classifier\n",
    "* SVM\n",
    "* Decision Tree (most popular and should be tried first)\n",
    "\n",
    "Decision trees are highly accurate and easy to use. They are invariant to input scale, and get good performance with little tuning.\n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "* AdaBoost can create a forest sequentially\n",
    "* But can we create the ensemble in parallel? Not by training subsets and bagging, because the trees will end with similar splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
       "0             5.1          3.5           1.4          0.2     Setosa\n",
       "1             4.9          3.0           1.4          0.2     Setosa\n",
       "2             4.7          3.2           1.3          0.2     Setosa\n",
       "3             4.6          3.1           1.5          0.2     Setosa\n",
       "4             5.0          3.6           1.4          0.2     Setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  Virginica\n",
       "146           6.3          2.5           5.0          1.9  Virginica\n",
       "147           6.5          3.0           5.2          2.0  Virginica\n",
       "148           6.2          3.4           5.4          2.3  Virginica\n",
       "149           5.9          3.0           5.1          1.8  Virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = pandas.read_csv(\"iris.csv\")\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain feature importance\n",
    "\n",
    "* Weight or split - The number of times a feature is used to split the data across all trees\n",
    "* Gain - Average gain of the feature when it is used in trees\n",
    "\n",
    "## Tuning hyperparameters\n",
    "\n",
    "* Every software model has tunable parameters\n",
    "* A better score on the project requires parameter tuning\n",
    "* Cross validation is used for this purpose. Part of the training set is used to mimick the test set, a so-called validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture hour 2\n",
    "\n",
    "* Regression trees have real numbers in the leave nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunable parameters:\n",
    "\n",
    "* Bootstrap - with or without replacement\n",
    "* max_depth - max number of levels in each decision tree\n",
    "* max_features - max number of features considered for splitting a node\n",
    "* min_samples_leaf - min data points allowed in leaf node\n",
    "* min_samples_split - minimum of data points placed in node before split\n",
    "* n_estimators - number of trees in forest\n",
    "\n",
    "A grid search will become quite expensive. We can use:\n",
    "\n",
    "1. A random grid search\n",
    "2. A coarse grid search\n",
    "3. A fine grid search\n",
    "\n",
    "**We should do this in the project.**\n",
    "\n",
    "Read [graduate student descent](https://sciencedryad.wordpress.com/2014/01/25/grad-student-descent/).\n",
    "\n",
    "Random forest advantages:\n",
    "\n",
    "* No need for feature normalization\n",
    "* Parallel training\n",
    "* Widely used\n",
    "* Perform reasonably well with default parameters (quite important advantage)\n",
    "* Random forest is often the first choice for prototyping\n",
    "\n",
    "How to get from 99% to 99.9%?\n",
    "\n",
    "* Gradient Tree Boosting (1999)\n",
    "* Gradient Tree Boosting with Regularization\n",
    "    * Regularized Greedy Forest\n",
    "    * XGBoost\n",
    "    * LightGBM\n",
    "    * One more\n",
    "    \n",
    "**What is GXBoost?**\n",
    "\n",
    "* Is software, not only a model\n",
    "* Regularized objective for better model\n",
    "* Out of core computing\n",
    "* Cache optimization\n",
    "* Distributed computing\n",
    "* Sparse aware algorithm\n",
    "* Weighted approximate quantile sketch\n",
    "* More than half teams in Kaggle uses XGBoost\n",
    "* Many industrial applications use XGBoost or its variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# We should all install xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(...)\n",
    "dtest = xgb.DMatrix(...)\n",
    "\n",
    "...\n",
    "\n",
    "model = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "xgb.plot_importance(model, max_num_features=15)\n",
    "plt.show()\n",
    "\n",
    "xgb.plot_tree(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost learns the best direction for misisng values! It creates default directions, directions which are found during training.\n",
    "* It offers a speedup for sparse data, such as categorical encodings and other cases, for example bag of words.\n",
    "* Out-of-core (disk-based) runs, approximate algorithm for split finding\n",
    "* Can customize the loss function\n",
    "* Can enable early-stopping\n",
    "* Can set checkpoints\n",
    "* Etc.\n",
    "\n",
    "## Parameter tuning in XGBoost\n",
    "\n",
    "* The default settings usually does not work well, and it is tricky in XGBoost\n",
    "* There are quite a lot of parameters, way too many actually\n",
    "* The complete lists can be found by searching \"XGBoost parameters\"\n",
    "* \"complete guide parameter tuning xgboost with codes python\" on analyticsvidhya\n",
    "    * First choose a relatively high learning rate\n",
    "    * Fix the learning rate\n",
    "    * Tune tree-specific parameters\n",
    "    * More...\n",
    "* Bayesian optimization can be used in the parameter tuning. Gradient descent can't be used since we don't have the derivative. Read the philipperemy visualiziation blog post on github.io.\n",
    "* There is also a kaggle kernel in the slides by \"nanomathias\"\n",
    "* **Remember to use stratified cross-validation!**\n",
    "\n",
    "## LightGBM (frontier)\n",
    "\n",
    "* GBDT requires all data in each boosting step, and this is too expensive for large data sets\n",
    "* Previous workarounds:\n",
    "    * uniform downsampling to get a subset for training, but this looses information.\n",
    "    * XGBoost uses histograms in order to remove some information\n",
    "* LightGBM tries to downsample in a smarter way\n",
    "* AdaBoost has sample weight, but GBDT has none weights that can be used\n",
    "* In LightGBM, leaf with higher gradient/error is used for growing further\n",
    "\n",
    "## CatBoost (frontier)\n",
    "\n",
    "* Overcomes the target leakage in category features and boosting. Target leakage is when the target meas wrongly using the responses, which theoretically leads to wrong fitting. I did not understand this, must read up on the concept...\n",
    "* We usually apply one-hot encoding before using XGBoost, but it becomes problematic when the number of categories is large.\n",
    "* Uses \"ordered target statistics\" instead.\n",
    "    * First it shuffles the data (rows) and TS is calculated using the other responses before the data point\n",
    "    * Proven to be no target leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lg = lgb.LGBMClassifier(silent=False)\n",
    "lgb.Dataset(...)\n",
    "\n",
    "import catboost as cb\n",
    "cb_classifier = cb.CatBoostClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CatBoost can perform better and can be trained faster\n",
    "\n",
    "## Choice of stacking bases\n",
    "\n",
    "This is the priority on the project\n",
    "\n",
    "#. XGBoost, LightGBM, CatBoost\n",
    "#. Random forest, AdaBoost, KNN\n",
    "#. DNN, LegReg, SVM, Gaussian Process (for regression)\n",
    "#. Fisher's linear discriminant, Naive Bayes\n",
    "\n",
    "## Exercise\n",
    "\n",
    "* Optical digits (Optical Recognition of Handwritten Digits Data Set - archive.ics.uci.edu)\n",
    "* Optional: IJCNN (csie.ntu.edu.tw)\n",
    "\n",
    "## Practical information\n",
    "\n",
    "* The last lecture has been changed and the project deadline has changed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
